# Simplified RLM Implementation

This directory contains a minimal, educational implementation of the Recursive Language Model (RLM) concept. It is designed to be easy to understand and modify, stripping away the complexity of multi-process communication, sandboxing, and advanced logging found in the full library.

## Core Components

1.  **`rlm.py` (The Agent):**
    *   Manages the loop: Prompt -> LLM -> Code Execution -> Observation -> Repeat.
    *   Maintains conversation history.
    *   Parses code blocks from LLM responses.

2.  **`environment.py` (The World):**
    *   A persistent Python REPL (Read-Eval-Print Loop).
    *   Executes code generated by the LLM.
    *   Maintains state (variables) across iterations.
    *   Injects the `llm_query()` function, enabling the model to call itself (recursion).

3.  **`llm.py` (The Intelligence):**
    *   Defines the `LLMClient` interface.
    *   Includes a `MockLLMClient` for testing without an API key.

## Usage

### Basic Example

```python
from simplified.llm import MockLLMClient # Or implement your own OpenAI wrapper
from simplified.rlm import RLM

# 1. Initialize the LLM Client
client = MockLLMClient()

# 2. Initialize the RLM
rlm = RLM(client, verbose=True)

# 3. Ask a question
answer = rlm.ask("What is the 10th fibonacci number?")
print(answer)
```

### Implementing a Real LLM Client

To use a real model (e.g., OpenAI), implement the `LLMClient` interface:

```python
import openai
from simplified.llm import LLMClient

class OpenAIClient(LLMClient):
    def __init__(self, api_key):
        self.client = openai.OpenAI(api_key=api_key)

    def completion(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

## Key Differences from Core Library

*   **No Socket Server:** Communication is direct function calls within the same process, not TCP sockets.
*   **No Sandboxing:** Code runs directly via `exec()`, which is unsafe for untrusted code but fine for local experiments.
*   **Simplified Context:** Globals and locals are merged into a single namespace.
*   **Synchronous:** Everything runs sequentially.
