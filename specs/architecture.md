# Architecture & Communication

## Overview
The RLM system consists of three main components: the **Controller (RLM)**, the **Language Model (Client)**, and the **Execution Environment (REPL)**.

## Components

### 1. Controller (RLM)
*   **Role**: Orchestrates the interaction loop.
*   **Responsibilities**:
    *   Manages the conversation history.
    *   Calls the LLM Client.
    *   Parses LLM responses to find code blocks.
    *   Sends code to the Environment for execution.
    *   Receives execution results and updates history.

### 2. Execution Environment (REPL)
*   **Role**: Executes the code generated by the LLM.
*   **Responsibilities**:
    *   Maintains persistent state (variables) across iterations.
    *   Provides access to the `context` data.
    *   Provides special functions (`llm_query`) to call back to the LLM.
    *   Captures `stdout` and `stderr`.
*   **Isolation**:
    *   *Local*: Runs in the same process or a subprocess.
    *   *Isolated*: Runs in a container or remote VM (e.g., Docker, Modal).

### 3. Language Model (Client)
*   **Role**: Generates text and code.
*   **Responsibilities**:
    *   Takes prompts and returns completions.
    *   Can be an external API (OpenAI, Anthropic) or a local model.

## Communication Flow

### Standard Loop
1.  **RLM -> Client**: `completion(history)`
2.  **Client -> RLM**: `response (text + code)`
3.  **RLM -> Environment**: `execute_code(code)`
4.  **Environment -> RLM**: `result (stdout/stderr)`

### Recursive Call (The "Magic")
When the executed code contains `llm_query(prompt)`:

1.  **Environment (Code) -> Environment (System)**: Calls the injected `llm_query` Python function.
2.  **Environment -> LLM Handler**:
    *   *In-Process (Simplified)*: Direct function call to the Client instance.
    *   *Out-of-Process (Production)*: TCP/HTTP request to a server managed by the RLM Controller.
3.  **LLM Handler -> Client**: `completion(prompt)` (Note: This is a fresh call, usually with depth+1).
4.  **Client -> LLM Handler**: `response`
5.  **LLM Handler -> Environment**: `response`
6.  **Environment**: Returns the response to the executing code.

## Protocol (Production)
In the full version, the Environment and the RLM Controller often run in different processes/machines.
*   **TCP/Socket**: Used for low-latency communication between a local subprocess REPL and the Controller.
*   **HTTP**: Used for communicating with remote sandboxes (e.g., Modal).

## Simplified Protocol
For the simplified version, we can run the Environment in the same process.
*   **Direct Call**: The `llm_query` function in the `exec` scope effectively calls `self.client.completion()` directly.
*   **Note**: This blocks the execution thread, which is fine for synchronous execution.
