# Architecture

The simplified RLM system consists of three main components:

## 1. LLM Client (`LLMClient`)

*   **Role:** interfaces with the underlying Language Model provider (e.g., OpenAI, Mock).
*   **Responsibility:** Sends prompts to the model and returns text completions.
*   **Simplified Logic:** Direct synchronous calls. No complex routing or socket servers.

## 2. Environment (`Environment`)

*   **Role:** Executes the code generated by the LLM.
*   **Responsibility:**
    *   Maintains a dictionary of global variables (`globals`) representing the state.
    *   Executes Python code strings via `exec()`.
    *   Captures `stdout` and return values.
    *   Injects the `llm_query` function into the global scope so the code can call the LLM.
*   **Simplified Logic:** Runs in the same process using `exec`. Captures `sys.stdout`.

## 3. RLM Agent (`RLM`)

*   **Role:** Orchestrates the loop.
*   **Responsibility:**
    *   Manages the conversation history.
    *   Constructs prompts (System prompt + History + User input).
    *   Parses the LLM's output to find code blocks or final answers.
    *   Passes code to the Environment.
    *   Updates history with execution results.
    *   Determines when to stop (max iterations or final answer found).

## Data Flow

1.  **User** calls `rlm.ask("Question")`.
2.  **RLM** creates initial prompt.
3.  **Loop**:
    *   **RLM** -> **LLMClient**: `completion(prompt)`
    *   **LLMClient** -> **RLM**: `response` (text with code)
    *   **RLM** parses code.
    *   **RLM** -> **Environment**: `execute(code)`
    *   **Environment** runs code:
        *   If `llm_query()` called: **Environment** -> **LLMClient** -> **Environment**
    *   **Environment** -> **RLM**: `result` (stdout/locals)
    *   **RLM** updates prompt with result.
4.  **RLM** returns final answer to **User**.
