# Core Concepts

## Recursive Language Models (RLMs)

An RLM is a system where a Large Language Model (LLM) acts as an agent that can:
1.  **Reason:** Generate thoughts and plans.
2.  **Act:** Execute code in an environment.
3.  **Recurse:** Call itself (or other models) as a function within that code to solve sub-problems.

Key differences from standard Agents:
*   **Code-First:** The primary interface for action is writing executable code (Python).
*   **Programmatic Recursion:** "Tool use" is generalized as calling `llm_query()` within the generated code. The model explicitly manages the decomposition and gathering of results through variables in the code.

## The REPL Environment

The Read-Eval-Print Loop (REPL) is the "world" the RLM inhabits.
*   **State:** It maintains variables (state) across iterations.
*   **Execution:** It executes code blocks generated by the RLM.
*   **Interface:** It provides special functions like `llm_query(prompt)` that allow the code to interact back with the intelligence layer.

## The Loop

The RLM operates in a loop:
1.  **Prompt:** The system presents the current state (conversation history, previous code execution results) to the LLM.
2.  **Generate:** The LLM generates a response, which may include thoughts and a code block.
3.  **Execute:** The system extracts the code block and executes it in the REPL environment.
    *   *Recursion:* If the code contains `llm_query()`, the environment handles this by calling the LLM (potentially spawning a sub-process or just a function call) and returning the result to the variable in the code.
4.  **Observe:** The output of the execution (stdout, stderr, variables) is captured.
5.  **Repeat:** The observation is added to the history, and the loop repeats until a final answer is produced.
